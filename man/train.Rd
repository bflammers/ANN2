% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/interface.R
\name{train}
\alias{train}
\title{Continue training of a Neural Network}
\usage{
train(object, X, Y, n.epochs = 500, learn.rates = 1e-04,
  momentum = 0.2, L1 = 0, L2 = 0, batch.size = 32,
  drop.last = TRUE, val.prop = 0.1, verbose = TRUE)
}
\arguments{
\item{object}{object of class \code{ANN}}

\item{X}{matrix with explanatory variables}

\item{momentum}{numeric value specifying how much momentum should be
used. Set to zero for no momentum, otherwise a value between zero and one.}

\item{L1}{L1 regularization. Non-negative number. Set to zero for no regularization.}

\item{L2}{L2 regularization. Non-negative number. Set to zero for no regularization.}

\item{verbose}{logical indicating if additional information (such as lifesign)
should be printed to console during training.}

\item{y}{matrix with dependent variables}

\item{learnRate}{the size of the steps made in gradient descent. If set too large,
optimization can become unstable. Is set too small, convergence will be slow.}

\item{maxEpochs}{the maximum number of epochs (one iteration through training
data).}

\item{batchSize}{the number of observations to use in each batch. Batch learning
is computationally faster than stochastic gradient descent. However, large
batches might not result in optimal learning, see Le Cun for details.}

\item{validLoss}{logical indicating if loss should be monitored during training.
If \code{TRUE}, a validation set of proportion \code{validProp} is randomly
drawn from full training set. Use function \code{plot} to assess convergence.}

\item{validProp}{proportion of training data to use for validation}
}
\value{
An \code{ANN} object. Use function \code{plot(<object>)} to assess
loss on training and optionally validation data during training process. Use
function \code{predict(<object>, <newdata>)} for prediction.
}
\description{
Train a Multilayer Neural Network using Stohastic Gradient
Descent with optional batch learning. Functions \code{autoencoder}
and \code{replicator} are special cases of this general function.
}
\details{
A genereric function for training Neural Networks for classification and
regression problems. Various types of activation and cost functions are
supported, as well as  L1 and L2 regularization. Additional options are
early stopping, momentum and the specification of a learning rate schedule.
See function \code{example_NN} for some visualized examples on toy data.
}
\examples{
# Example on iris dataset:
randDraw <- sample(1:nrow(iris), size = 100)
train    <- iris[randDraw,]
test     <- iris[setdiff(1:nrow(iris), randDraw),]

plot(iris[,1:4], pch = as.numeric(iris$Species))

NN <- neuralnetwork(train[,-5], train$Species, hiddenLayers = c(5, 5),
                    momentum = 0.8, learnRate = 0.001, verbose = FALSE)
plot(NN)
pred <- predict(NN, newdata = test[,-5])
plot(test[,-5], pch = as.numeric(test$Species),
     col = as.numeric(test$Species == pred$predictions)+2)

#For other examples see function example_NN()

}
\references{
LeCun, Yann A., et al. "Efficient backprop." Neural networks:
Tricks of the trade. Springer Berlin Heidelberg, 2012. 9-48.
}
